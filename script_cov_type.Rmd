---
title: "Cover_Type"
author: "Lucia, Lucile et Youna"
date: "2023-11-03"
output: html_document
---

# Introduction et présentation des données
## Problématisation

Dans cette étude, il s'agit d'utiliser le machine learning comme outil pour prédire la couverture forestière présente sur un territoire. L'étude poursuite celle réalisée dans un article publié en 1998 par Blackard et Dean. Les données ont été prélevée dans le parc National Roosevelt (Colorado). Cette problématique de prédiction de la couverture végétale est capitale pour les gestionnaires de territoires. Le machin learning permet d'avoir une idée de cette dernière sans avoir besoin de se déplacer sur le terrain. Cela peut-être particulièrement couteux en cas de zone innaccessibles ou éloignées. Il est également possible d'obtenir des informations sur un territoire n'étant pas sous notre contrôle direct. 
Pour prédire cette couverture forestière (variable réponse), plusieurs variables quantitatives liées à la géographie et qualitative liées au type de sol sont regroupées dans un même tableau.

## Importation et formatage des données

Importation des library nécessaires
```{r, echo=false}
library(tidyverse)
library(dplyr)
library(FactoMineR)
library(ggplot2)
library(class)
library(ggplot2) 
library(corrplot)
library(glmnet) 
library(rpart)
library(partykit)
#library(caret)
library(randomForest)
library(randomForestExplainer)
```

Importation des données et modification des noms de colonnes.
```{r}
forest= read.table(file = "covtype.data", sep=",", header=TRUE) 

a <- rep("Soil_Type_",40)
for (i in 1:40) {
  a[i]<-paste0(a[i],as.character(i))
} 

name <- c("Elevation",
          "Aspect",
          "Slope",
          "Horizontal_Distance_To_Hydrology",
          "Vertical_Distance_To_Hydrology",
          "Horizontal_Distance_To_Roadways",
          "Hillshade_9am",
          "Hillshade_Noon",
          "Hillshade_3pm",
          "Horizontal_Distance_To_Fire_Points",
          "Wilderness_Area_1",
          "Wilderness_Area_2",
          "Wilderness_Area_3",
          "Wilderness_Area_4",
          a,
          "Cover_Type")

colnames(forest)<- name
```

Extraction des labels à prédire pour les apprentissages supervisés. Les labels sont stockés dans Cov_type. Xforest est le jeu de donné initial auquel est enlevé la colonne des labels.

```{r}
Cov_type = forest$Cover_Type
Xforest <- subset(forest, select = -Cover_Type)
```

Réduction des dimensions avec une PCA. Seul le premier plan est conservé pourles analyses nécessitant une réduction de dimension. 
```{r}
pcaXforest <- PCA(Xforest, ncp = 2) 
```

## Vérification des données

### Généralités

```{r}
#str(forest)
table(is.na(forest))
```
Il n'y a pas de valeurs manquantes et les variables sont au bon format:  
- factor pour les variables qualitatives,  
- int pour les variables quantitatives 

### Valeurs extremes de $X$ 

```{r dataCov1, include=TRUE}
par(mfrow=c(2,3))
for (i in 1:10) {
  # Cleveland plot
  boxplot(forest[,i],pch=16,col='salmon',xlab=colnames(forest)[i])
}

```

Les valeurs d'ombrages peuvent sembler très basses mais les points sans ombre à 9 heures ne sont pas les mêmes que ceux à 12 heures, les valeurs paraissent cohérentes.Les autres valeurs semblent plausibles.


### Répartition des individus pour les variables qualitatives 

```{r}
quali <- apply(forest[,11:54], MARGIN = 2, table)
print(as.data.frame(round(100*quali[2,]/(quali[1,]+quali[2,]),digits = 2),optional = TRUE ))
```

En calculant le pourcentage des observations appartenant à chaque classe de "Wilderness area" et de "Soil type, on remarque que Wilderness area 2 et 4 ne représentent respectivement que 5.1 et 6.4 % des observations.\
\

Pour les types de sols, on remarque que la plupart sont assez peu représentés avec un pourcentage de représentation de l'ordre de $10^{-1}$ ou $10^{0}$. Le type de sol numéro 29 est particulièrement représenté avec environ 20% des sols échantillonnés lui correspondant. Le sol le moins représenté est le sol 15 qui a un pourcentage de représentation de l'ordre de $10^{-4}$.

### Relations entre X & Y 

Première estimation des relations possibles entre les variables quantitatives explicatives et la variable réponse.

```{r dataCov2, include=TRUE, fig.height=3, fig.width=8}
# variables X quantitatives
par(mfrow=c(1,2))
for (i in 1:10) {
  boxplot(Xforest[,i]~Cov_type,ylab=colnames(Xforest)[i],xlab = "Cover type")
}

```

Sans analyses plus poussées on ne peut que faire des hypothèses sur les variables impactant Y. Ainsi, l'altitude et la distance à la route semblent jouer un rôle important pour discriminer les différents types de couverts végétaux.

# Implémentation des algorithmes de machine learning
## Algorithmes data driven 

### Non-spervisés

#### Algorithme des k-means
##### Principe de la méthode
La méthode des k-means est un clustering selon les similarité des individus au sein du groupe et des différence des individus d'autres groupes. Un idividu est affecté à un nouveau groupe s'il est plus proche de la moyenne de ce nouveau groupe que de la moyenne de son groupe initial. Les moyennes sont mise-à-jour à chaque itération.

Il faut initialement déterminer le nombre de groupes attendu. Ici, c'est le nombre de couverts forestiers possibles.

##### Algorithme des k-means pour prédire le type de couverture forestière 
```{r}
n_clusters = length(unique(Cov_type)) # Nombre de cluster attendu : autant que de couverts forestiers possibles
set.seed(1)
i.centers = sample(1:dim(pcaXforest$ind$coord)[1],n_clusters) # initialisation aléatoire des clusters. 
centers = pcaXforest$ind$coord[i.centers,]
km = kmeans(pcaXforest$ind$coord,centers,nstart=10,iter.max=50) # avec 10 sets aléatoires et 50 itérations au maximum
cl = km$cluster # tous les individus sont associés à une classe

cl_lab = cl
for (k in 1:n_clusters){
  ii = which(cl==k) 
  counts=table(Cov_type[ii]) # Nb d'occurences de chaque classe
  imax=which.max(counts) # Calcul de la classe majoritaire
  maj_lab=attributes(counts)$dimnames[[1]][imax] # attribution de l'étiquette de la classe
  print(paste("Classe ",k,", label majoritaire = ",maj_lab))
  cl_lab[ii] = maj_lab
}

#matrice de confusion et accuracy
confusion = (conf_mat = table(Cov_type,cl_lab, dnn=list("Observed","Predicted")))
print(confusion)
accuracy <- sum(cl_lab == Cov_type)/(length(Cov_type))
print(paste("l'accuracy de la classification avec l'algorithme des k-means est de",accuracy)) #0.5507693

# représentation de la matrice de confusion
df = as.data.frame(confusion)
df %>% 
  ggplot( aes(x = Observed, y = Predicted, fill = Freq))+
  geom_tile()+
  geom_text(aes(label = round(Freq, 1))) +
  scale_fill_gradient(low="#FFFF88",high="#FF0000")+
  coord_fixed()+
  theme_minimal()+
  theme(axis.text = element_text(size = 12))+
  theme(axis.title = element_text(size = 14)) 

```

L'accuracy est acceptable, plus de la moitié des couverts forestiers sont bien prédit. Néanmoins, La seule classe correctement prédite est la classe 2. Cela n'est pas étonnant car il s'agit d'une classe très majoritaire. Les classes 1, 2 et 3 sont les seules que l'algorithme parvient à proposer, encore une fois, cela est du au très faible nombre d'individus des classes 4 à 7, comme présenté dans l'histogramme ci-dessous :

```{r}
hist(Cov_type, col="yellow3")
```

Finalement, l'algorithme n'a pas su former des groupes correspondant aux Cover types car, dans chaques groupe, les classes 1, 2 et 3 sont majoritaire, du fait de leur sur-représentation.

### Supervised

Pour les apprentissages supervisés, il s'agit de commencer l'analyse en séparant le jeu de donné en deux set, train et test, pour d'abord entrainer l'algorithme puis l'évaluer sur des données différentes. La valeur choisie est généralement 2/3 de données d'entraînement et 1/3 de données test. Pour comparer avec la méthode model driven, un partitionnement particulier a été réalisé sur une plus faible partie du jeu de donnée.

Voici la fonction de partitionnement implémentée :

```{r}
part_data <- function(data,freq = rep(1/7,7)){
  # prend en entree le jeu de donnee à echantillonner et un vecteur de fréquences pour chaque classe dans l'ordre (1 à 7)
  # valeur par défaut, toutes les classes sont échantillonnées à peu près à la meme fréquence
  # renvoie les indices des individus de l'echantillon d'entrainement et de l'echantillon de validation
  nb_indiv <- length(data[,1])
  itrain <- c()
  itest <- c(1:nb_indiv)
  # on force l'echantillon d'entrainement à avoir au moins 3 observations par type de sol
  for (i in 15:54) { #les colonnes 14 à 53 sont les colonnes "soil type"
    itrain <- c(itrain,sample(x = which(data[,i] == 1 ),size = 3)) 
  }
  
  # on force l'echantillon d'entrainement à avoir au moins 3 observations par type de sol
  for (i in 11:14) { #les colonnes" 14 à 53 sont les colonnes "soil type""Wilderness_area"
    itrain <- c(itrain,sample(x = setdiff(which(data[,i] == 1 ),itrain),size = 3)) #dans les données pas deja echantilonnee 
  }
  
  # on force l'echantillon d'entrainement à avoir au moins 7000 observations dans les fréquences respectives de chaques classes
  nb <- freq*7000
  for (i in 1:7) {
    itrain <- c(itrain,sample(x = setdiff(which(data$Cover_Type == i),itrain),size = nb[i]))
  }
  
  itest <- setdiff(itest,itrain)
  itest <- sample(itest,3560) 
  
  return(list(itrain,itest))

}


# Fréquence de chaque classe
table(forest$Cov_Type)
freq_cover <- table(forest$Cov_Type)/length(forest$Cov_Type) # en proportions
```
#### Algorithme des k-nearest neighbourgs
##### Principe de la méthode

La méthode des K-NN consiste à réaliser une moyenne entre les individus voisins pour estimer la variable d'intérêt. Le nombre optimal de voisin doit être identifié car l'algorithme est sensible à ce choix. Comme il s'agit de classification, la réponse est estimée au vote majoritaire afin de prédire la classe.

##### Algorithme des k-nearest neighbourgs pour prédire une couverture forestière, avec le partitionnement présenté

Recherche du meilleur nombre de voisins :
```{r}
#on travaille sur la dimention réduite avec l'objet pcaXforest
C = factor(Cov_type,levels=c(1,2,3,4,5,6,7)) 
nb_Cov_type <- length(unique(Cov_type))

# on va trouver le meilleur nombre de voisin possible pour minimiser l'erreur
nb_neighbors = c(1,10,20,50,100,150,200)

n = dim(pcaXforest$ind$coord)[1]
B = 10
err_knn = matrix(0,B,length(nb_neighbors))


for (b in 1:B) { #10 répétitions du modèle 
  set.seed(b)
  
  itrain <- unlist(part_data(forest, freq_cover)[1])
  itest <- unlist(part_data(forest, freq_cover)[2])
  
  
  Xs_train = scale(pcaXforest$ind$coord[itrain,])
  Xs_test = scale(pcaXforest$ind$coord[itest,],center=apply(pcaXforest$ind$coord[itrain,],2,mean),scale=apply(pcaXforest$ind$coord[itrain,],2,sd))
  for (i in 1:length(nb_neighbors)) {
    knn_res = knn(Xs_train,Xs_test,Cov_type[itrain],k=nb_neighbors[i],prob=TRUE)
    err_knn[b,i] = sum(Cov_type[itest]!=knn_res)/length(itest)
  }
  
}

apply(err_knn,2,mean) #on fait la moyenne des col de err_knn 
colnames(err_knn)=nb_neighbors
boxplot(err_knn) 
w = which.min(apply(err_knn,2,mean)) #le meilleur nombre de voisins
print(paste("le nombre de voisins optimal est",nb_neighbors[w]))
```

Application de l'algorithme aux données avec le nombre de voisins optimal, avec le partitionnement du jeu de données

```{r}
w = which.min(apply(err_knn,2,mean)) #le meilleur nombre de voisins

n = dim(pcaXforest$ind$coord)[1]
B = 10 # number of samples for the cross validation
y_test = y_prob = NULL
for (b in 1:B){
  set.seed(b)
  
  itrain <- unlist(part_data(forest, freq_cover)[1])
  itest <- unlist(part_data(forest, freq_cover)[2])
  
  Xs_train = scale(pcaXforest$ind$coord[itrain,])
  Xs_test = scale(pcaXforest$ind$coord[itest,],center=apply(pcaXforest$ind$coord[itrain,],2,mean),scale=apply(pcaXforest$ind$coord[itrain,],2,sd))
  knn_res = knn(Xs_train,Xs_test,C[itrain],k=nb_neighbors[w],prob=TRUE)
  y_prob = c(y_prob,attributes(knn_res)$prob)
  y_test = c(y_test,C[itest])
}

Cres <- C[itest]

conf_mat_knn = (table(Cres, knn_res, dnn=list("Predicted", "Observed")))
print(conf_mat_knn)

accuracy <- sum(knn_res == Cres)/(length(Cres))
print(accuracy)

# représentation de la matrice de confusion
df = as.data.frame(conf_mat_knn)
df %>% 
  ggplot( aes(x = Observed, y = Predicted, fill = Freq))+
  geom_tile()+
  geom_text(aes(label = round(Freq, 1))) +
  scale_fill_gradient(low="#FFFF88",high="#FF0000")+
  coord_fixed()+
  theme_minimal()+
  theme(axis.text = element_text(size = 12))+
  theme(axis.title = element_text(size = 14)) 
```

##### Algorithme des k-nearest neighbourgs pour prédire une couverture forestière, avec le partitionnement 2/3 et 1/3

Application de l'algorithme aux données avec le nombre de voisins optimal. Cette fois, l'ensemble du jeu de données est utilisé. Attention, très gourmand en temps de calcul.

```{r}
w = which.min(apply(err_knn,2,mean)) #le meilleur nombre de voisins

n = dim(pcaXforest$ind$coord)[1]
B = 10 # number of samples for the cross validation
y_test = y_prob = NULL
for (b in 1:B){
  set.seed(b)
  
  itrain <- sample(c(1:nrow(forest)), round((2/3)*n))
  itest <- setdiff(c(1:nrow(forest)), itrain)
  
  Xs_train = scale(pcaXforest$ind$coord[itrain,])
  Xs_test = scale(pcaXforest$ind$coord[itest,],center=apply(pcaXforest$ind$coord[itrain,],2,mean),scale=apply(pcaXforest$ind$coord[itrain,],2,sd))
  knn_res = knn(Xs_train,Xs_test,C[itrain],k=25, prob=TRUE)#nb_neighbors[w],prob=TRUE)
  y_prob = c(y_prob,attributes(knn_res)$prob)
  y_test = c(y_test,C[itest])
}


Cres <- C[itest]

conf_mat_knn = (conf_mat = table(Cres, knn_res))
print(conf_mat_knn)

accuracy <- sum(knn_res == Cres)/(length(Cres))
print(accuracy)

df = as.data.frame(conf_mat_knn)
df %>% 
  ggplot( aes(x = Observed, y = Predicted, fill = Freq))+
  geom_tile()+
  geom_text(aes(label = round(Freq, 1))) +
  scale_fill_gradient(low="#FFFF88",high="#FF0000")+
  coord_fixed()+
  theme_minimal()+
  theme(axis.text = element_text(size = 12))+
  theme(axis.title = element_text(size = 14)) 

```

Le résultat est à peine meilleur lorsque l'ensemble du jeu de donnée est utilisé et est séparé en 2/3 de train et 1/3 de test. En effet, il s'agit d'une méthode datadriven, la solution doit donc "émerger" des données et pour celà, les données doivent être en quantité. Les résultats obtenus avec la fonction de partitionnement réalisée sont déjà très satisfaisants car cette dernière représente bien le jeu de données initial.

## Arbre de décision
### Introduction
Dans un second temps, nous allons utiliser des data driven models mais cette fois-ci sans réduction de la dimension. Nous allons mettre en oeuvre un algorithme d'arbre de décision, notamment pour la facilité d'implémentation et d'interprétation. Il s'agit ici en l'occurence d'un arbre de classification puisque la variable à prédire est qualitative. Dans un but d'abord exploratoire, on réalisera les simulations avec un sous jeu de données construit pour être représentatif du jeu de données initial (fonction part_data), qui entrainera des temps de simulations moins élevés. Afin de prévenir et détecter du sur-apprentissage on mettra en place une méthode de validation croisée en simulant l'algorithme N fois avec des jeu d'apprentissages et de test tirés au hasard dans le sous jeu de données. 
  
Seules les classes 1 et 2 sont bien prédites, soit les classes les plus représentées. 
La méthode n'est plutôt satisfaisante : accuracy de 0.57 avec le partionnement proposé l'accuracy est la même  avec les tiers. Cela s'explique par le grand déséquilibre dans les données et la sous-représentation de certaines classes par rapport aux autres.

### Simulation
On commence d'abord par la simulation d'un arbre peu profond avec le paramètre par défaut cp=0.1

```{r}
N=10

forest_shuffled <- forest[sample(nrow(forest)), ] #Mélange aléatoire
accuracy_scores <- numeric(N) #Initiation du vecteur d'accuracy

for (b in 1:N) { #Boucle pour la validation croisée
  res=part_data(forest,freq_cover) #Sélection d'un sous jeu de données représentatif
  itest=res[[2]] #Récuperation des indices des lignes pour le jeu de test
  itrain=res[[1]]#Récuperation des indices des lignes pour le jeu d'entrainement
  forest_test <-forest[itest,] #Création du jeu de test
  forest_train <-forest[itrain,] #Création du jeu d'entrainement

  #Exctraction des variables explicatives
  X_forest_test <-forest_test[,1:54] 
  X_forest_train <-forest_train[,1:54]
  
  #Extraction de la variable à prédire
  Y_forest_test<-forest_test$Cover_Type
  Y_forest_train <-forest_train$Cover_Type
  
  #Algorithme de l'arbre de décision
  tree_model <- rpart(Cover_Type~., data = forest_train, method="class",control = rpart.control(cp = 0.1)) 
  
  #La fonction rpart contrôle permet de créer des arbres plus profond lorsque cp est faible, tout en pénalisant la compléxité pour prévenir le surapprentissage.
  
  # Prédiction sur jeu de test à partir du modèle
  y_pred <- predict(tree_model,forest_test, type='class')
  
  #Récupération du score de précision
  accuracy_scores[b] <- sum(y_pred == Y_forest_test)/ length(Y_forest_test)

}


```


## Visualisation de l'arbre de décision
```{r}
  op = par(mfrow=c(1,2), cex=0.5)
#Affichage de l'arbre
  plot(tree_model, uniform = TRUE, branch = 0.5, margin = 0.2)
  text(tree_model, all = FALSE, use.n = FALSE)
#Relation erreur et valeur de cp
  plotcp(tree_model)
```

On observe un arbre très simple, les prédictions sont faites à partir d'une seule variable Elévation, parmi les 54 variables explicatives possibles et il y a un seul noeud. Ainsi, seules deux classes sont prédites. De plus, sur l'autre graphique, on constate que l'erreur de prédiction n'atteint pas un palier avec cette valeur de cp, elle n'est donc pas minimale.

### Résultats
```{r}
#Matrice de confusion pour le dernier arbre
confusion0 = table(Y_forest_test,y_pred,dnn=list("Observed","Predicted"))

df0 = as.data.frame(confusion0)
df0 %>% 
  ggplot( aes(x = Observed, y = Predicted, fill = Freq))+
  geom_tile()+
  geom_text(aes(label = round(Freq, 1))) +
  scale_fill_gradient(low="#FFFF88",high="#FF0000")+
  coord_fixed()+
  theme_minimal()+
  ggtitle("Matrice de confusion decision tree")+
  theme(axis.text = element_text(size = 12))+
  theme(axis.title = element_text(size = 14))+
  theme(plot.title = element_text(hjust = 0.5))


mean_accuracy <- mean(accuracy_scores)
cat("La moyenne de la précision de l'algorithme après",N, "itération est de", mean_accuracy, "\n")


```
On constate que le score de précision est plutôt correct, sur les 10 itérations on obtient environ 60% de prédictions correctes. Cependant, lorsque l'on analyse la matrice de confusion, on constate que seules les 2 premières classes, les classes majoritaires, sont prédites. Les classes minoritaires 6 et 7 sont souvent confondues avec celles-ci, probablement car elles se ressemblent du point de vue de beaucoup de variables et que l'arbre n'est pas capable de distinguer ces détails.

Pour continuer, nous allons choisir une valeur plus faible pour le paramètre cp. Cela forcera l'algorithme à utiliser plus de variables et ainsi construire un arbre plus profond et un modèle plus précis. 

### Simulation
```{r}
N=10

forest_shuffled <- forest[sample(nrow(forest)), ] #Mélange aléatoire
accuracy_scores <- numeric(N) #Initiation du vecteur d'accuracy

for (b in 1:N) { #Boucle pour la validation croisée
  res=part_data(forest,freq_cover) #Sélection d'un sous jeu de données représentatif
  itest=res[[2]] #Récuperation des indices des lignes pour le jeu de test
  itrain=res[[1]]#Récuperation des indices des lignes pour le jeu d'entrainement
  forest_test <-forest[itest,] #Création du jeu de test
  forest_train <-forest[itrain,] #Création du jeu d'entrainement

  #Exctraction des variables explicatives
  X_forest_test <-forest_test[,1:54] 
  X_forest_train <-forest_train[,1:54]
  
  #Extraction de la variable à prédire
  Y_forest_test<-forest_test$Cover_Type
  Y_forest_train <-forest_train$Cover_Type
  
  #Algorithme de l'arbre de décision
  tree_model <- rpart(Cover_Type~., data = forest_train, method="class",control = rpart.control(cp = 0.0005)) 
  
  #La fonction rpart contrôle permet de créer des arbres plus profond lorsque cp est faible, tout en pénalisant la compléxité pour prévenir le surapprentissage.
  
  # Prédiction sur jeu de test à partir du modèle
  y_pred <- predict(tree_model,forest_test, type='class')
  
  #Récupération du score de précision
  accuracy_scores[b] <- sum(y_pred == Y_forest_test)/ length(Y_forest_test)

}

```

### Résultats
```{r}
  plotcp(tree_model)
```
On constate déja que la valeur cp = 0.0005, permet d'atteindre un palier et donc de réduire l'erreur de prédiction. L'arbre en lui même n'est plus interprétable puisqu'il est trop complexe.

```{r}
#Matrice de confusion pour le dernier arbre
confusion1 = table(Y_forest_test,y_pred,dnn=list("Observed","Predicted"))

df1 = as.data.frame(confusion1)
df1 %>% 
  ggplot( aes(x = Observed, y = Predicted, fill = Freq))+
  geom_tile()+
  geom_text(aes(label = round(Freq, 1))) +
  scale_fill_gradient(low="#FFFF88",high="#FF0000")+
  coord_fixed()+
  theme_minimal()+
  ggtitle("Matrice de confusion decision tree")+
  theme(axis.text = element_text(size = 12))+
  theme(axis.title = element_text(size = 14))+
  theme(plot.title = element_text(hjust = 0.5))

mean_accuracy <- mean(accuracy_scores)
cat("La moyenne de la précision de l'algorithme après",N, "itération est de", mean_accuracy, "\n")

```
On observe de meilleures performances de prédictions, 71% de prédictions correctes et toutes les classes sont prédites. On relève tout de même des difficultés à prédire les classes minoritaires.

Il est maintenant intéressant de comparer ces résultats issus de la simulation du sous jeu de données aux résultats avec toutes les données. On réalise cette fois une autre forme de validation croisée où l'on réalise N simulations du modèle en prenant au hasard 2/3 du jeu en données d'entraînement. Le reste des données sont utilisées pour tester le modèle. 

### Simulation
```{r}
N=10

accuracy_scores <- numeric(N) #Initiation du vecteur d'accuracy

for (b in 1:N) { #Boucle pour la validation croisée
  forest_shuffled <- forest[sample(nrow(forest)), ] #Mélange aléatoire
  
  forest_test <-forest[400001:581011,] #Création du jeu de test
  forest_train <-forest[1:400000,] #Création du jeu d'entrainement

  #Exctraction des variables explicatives
  X_forest_test <-forest_test[,1:54] 
  X_forest_train <-forest_train[,1:54]
  
  #Extraction de la variable à prédire
  Y_forest_test<-forest_test$Cover_Type
  Y_forest_train <-forest_train$Cover_Type
  
  #Algorithme de l'arbre de décision
  tree_model <- rpart(Cover_Type~., data = forest_train, method="class",control = rpart.control(cp = 0.0005)) 
  
  #La fonction rpart contrôle permet de créer des arbres plus profond lorsque cp est faible, tout en pénalisant la compléxité pour prévenir le surapprentissage.
  
  # Prédiction sur jeu de test à partir du modèle
  y_pred <- predict(tree_model,forest_test, type='class')
  
  #Récupération du score de précision
  accuracy_scores[b] <- sum(y_pred == Y_forest_test)/ length(Y_forest_test)

}

```

### Résultats
```{r}
#Matrice de confusion pour le dernier arbre
confusion1bis = table(Y_forest_test,y_pred,dnn=list("Observed","Predicted"))

df1bis = as.data.frame(confusion1bis)
df1bis %>% 
  ggplot( aes(x = Observed, y = Predicted, fill = Freq))+
  geom_tile()+
  geom_text(aes(label = round(Freq, 1))) +
  scale_fill_gradient(low="#FFFF88",high="#FF0000")+
  coord_fixed()+
  theme_minimal()+
  ggtitle("Matrice de confusion decision tree")+
  theme(axis.text = element_text(size = 12))+
  theme(axis.title = element_text(size = 14))+
  theme(plot.title = element_text(hjust = 0.5))

mean_accuracy <- mean(accuracy_scores)
cat("La moyenne de la précision de l'algorithme après",N, "itération est de", mean_accuracy, "\n")
```
Les performances sont encore meilleures avec 87% de prédictions correctes et toutes les classes prédites.

### Conclusion
L'analyse avec le sous jeu de données nous a permis de calibrer le paramètre cp avec des temps de simulations faibles. Mais on constate que les performances de prédictions sont meilleures lorsque l'on utilise tout le jeu de données, ce qui était attendu pour ce data driven model.

## Random forest
Afin d'obtenir encore plus de précision dans les prédictions, on utilise maintenant un algorithme similaire mais plus élaboré. L'algorithme des forêts aléatoires permet de combiner les arbres de décision à chaque étape. 
Le choix des paramètres de cette fonction est central
pour  obtenir de bonnes performances de prédiction sans entraîner de surapprentissage. 
  
Par manque de temps, nous n'avons pas utilisé les algorithmes qui optimiseraient la valeur des paramètres. Tout d'abord nous avons fixé les valeurs de minsplit (nombre de noeud minimum) et maxdepth (profondeur maximum) à des valeur qui correspondent globalement à la forme de l'arbre obtenu précedement avec cp=0.0005, soit maxdepth=5 et minsplit=15.
Nous avons ensuite procédé par tâtonnement avec le sous jeu de données pour fixer les valeurs de mtry et ntree. Pour une simulation, un nombre d'arbre, "ntree", seront construits à partir d'un échantillon bootstrap du jeu de données train avec un nombre, "mtry", de variables, choisi aléatoirement parmis les 54 variables explicatives. Ce paramètre mtry permet de minimiser la corrélation entre les arbres d'une forêt.  
Nous avons ainsi testé plusieurs combinaisons de valeurs de mtry et ntree afin de faire le choix le plus parcimonieux entre précision de prédiction et temps de simulation. Nous avons conclut que prendre 20 variables pour chaques simulations est suffisant (ce nombre correspond aux ordres de grandeurs trouvés dans la littérature) et qu'il n'était pas nécessaire de choisir ntree au delà de 20 (avec ntree = 50, on a la même précision mais un temps de simulation plus long).

Le code ci-dessous réalise la simulation avec les valeurs de paramètres sélectionés. 

### Simulation
```{r}

for (nb_var in 11:55){
  forest1 <- forest %>% 
    mutate(across(colnames(forest)[nb_var], as.factor))
}

N=10 #Nombre d'itérations
forest_shuffled <- forest1[sample(nrow(forest)), ] #Mélange aléatoire
accuracy_scores <- numeric(N)

for (b in 1:N) {  
  res=part_data(forest1,freq_cover)
  itest=res[[2]]
  itrain=res[[1]]
  forest_test <-forest1[itest,] #Test
  forest_train <-forest1[itrain,] #Apprentissage

  X_forest_test <-forest_test[,1:54]
  X_forest_train <-forest_train[,1:54]
  
  Y_forest_test<-forest_test$Cover_Type
  Y_forest_train <-forest_train$Cover_Type
  
  #Entrainement du modèle
  rf <-randomForest(Cover_Type~.,data=forest_train,ntree=20,mtry=20,control=rpart.control(maxdepth=5, minsplit=15))
  
#Prédiction sur le jeu de données test
  y_pred <- predict(rf,forest_test, type='class')
  
#Calcul de l'accuracy
  accuracy_scores[b] <- sum(y_pred == Y_forest_test)/ length(Y_forest_test)
  
}

```

### Résultats
```{r}

#Matrice de confusion pour le dernier arbre
confusion2 = table(Y_forest_test,y_pred,dnn=list("Observed","Predicted"))

df2 = as.data.frame(confusion2)
df2 %>% 
  ggplot( aes(x = Observed, y = Predicted, fill = Freq))+
  geom_tile()+
  geom_text(aes(label = round(Freq, 1))) +
  scale_fill_gradient(low="#FFFF88",high="#FF0000")+
  coord_fixed()+
  theme_minimal()+
  ggtitle("Matrice de confusion random forest")+
  theme(axis.text = element_text(size = 12))+
  theme(axis.title = element_text(size = 14))+
  theme(plot.title = element_text(hjust = 0.5))

mean_accuracy <- mean(accuracy_scores)
cat("La moyenne de la précision de l'algorithme après",N, "itération est de", mean_accuracy, "\n")

```

Avec cette algorithme, on obtient 75% de prédictions correctes mais on observe la même tendance: une difficulté à prédire les classes minoritaires. 

Cependant, l'algorithme des Random Forest nous donne accès à d'autres informations.

```{r}
op = par(mfrow=c(1,2), cex=0.5)
#Lien OOB et ntree
plot(rf$err.rate[, 1], type = "l", xlab = "nombre d'arbres", ylab = "erreur OOB")

#Importance des variables explicatives
plot(rf$importance)
rf$importance

```

  Sur le premier graphique, qui explicite la relation entre l'erreur out-of-bag et la valeur du paramètre ntree, on constate que cette erreur est plutôt bien minimisée par la valeur de ntree choisie.  
  On a ensuite accès à l'importance de chaque variables explicatives. Cette valeur d'importance est calculée en mesurant la différence d'accuracy à la suite de test de permutations entre les variables explicatives choisies. D'après le graphiques, les 20 premières variables explicatives sont les plus importante pour prédire correctement la variable Cover_Type. On prétend à une relation linéaire entre ces importances et ainsi déduire que la variable Elevation est 3 à 5 fois plus importante que les autres variables. Pour les reste, les autres variables quantitative et le facteur Wilderness area sont aussi relativement importants. En revanche, les variables Sol_type semble peu participer à l'analyse.
  Ce résultat est intéressant puisqu'il peut être utile si l'on souhaite réduire la dimension du jeu de données pour d'autre type d'analyse (KNN, Regression ...)


Pour finir, on semble avoir trouvé l'algorithme le plus performant pour prédire les Cover_Type en terme d'accuracy. Nous allons ainsi simuler cet algorithme avec la totalité du jeu de données pour essayer d'augmenter la précisions des prédictions et tirer des conclusions robustes.

### Random Forest: jeu de données complet
On réalise cette fois une autre forme de validation croisée ou l'on réalise N simulation du modèle en prenant au hasard 2/3 du jeu en données d'entrainement et le reste des données pour le test. 

```{r}
N=5 #Nombre d'itérations

forest_shuffled <- forest1[sample(nrow(forest1)), ] 
accuracy_scores <- numeric(N)
accuracy_scores_overfit <- numeric(N)

for (b in 1:N) { 
  set.seed(b)
  forest_shuffled <- forest1[sample(nrow(forest1)), ] 
  forest_apprentissage <- forest_shuffled[1:400000, ] #Apprentissage
  forest_test <- forest_shuffled[400001:581011, ]  #Test
  
  
  # Préparation des données pour l'apprentissage
  Y <- forest_apprentissage$Cover_Type
  X <- forest_apprentissage[,1:54]

  #Préparation des données pour le test
  Y2 <- forest_test$Cover_Type
  X2 <- forest_test[,1:54]
  
  #Entrainement du modèle
  rf <-randomForest(Cover_Type~.,data=forest_apprentissage,ntree=20,mtry=20,control=rpart.control(maxdepth=5, minsplit=15))

  #Prédiction sur le jeu de données test
  y_pred <- predict(rf,forest_test, type='class')
  
    #Prédiction sur le jeu de données d'apprentissage
  y_pred_overfit <- predict(rf,forest_apprentissage, type='class')

  #Calcul des accuracy
  accuracy_scores[b] <- sum(y_pred == Y2)/ length(Y2)
  accuracy_scores_overfit[b] <- sum(y_pred_overfit == Y)/ length(Y)

}
```

### Résulats
```{r}
confusion3 = table(Y2,y_pred,dnn=list("Observed","Predicted"))
df3 = as.data.frame(confusion3)
df3 %>% 
  ggplot( aes(x = Observed, y = Predicted, fill = Freq))+
  geom_tile()+
  geom_text(aes(label = round(Freq, 1))) +
  scale_fill_gradient(low="#FFFF88",high="red")+
  coord_fixed()+
  theme_minimal()+ggtitle("Matrice de confusion random forest")+
  theme(axis.text = element_text(size = 12))+
  theme(axis.title = element_text(size = 14))+
  theme(plot.title = element_text(hjust = 0.5))

mean_accuracy <- mean(accuracy_scores)
cat("Pour le jeu de données test, la moyenne de la précision de l'algorithme après",N, "itération est de", mean_accuracy, "\n")

mean_accuracy_overfit <- mean(accuracy_scores_overfit)
cat("Pour le jeu de données d'apprentissage, la moyenne de la précision de l'algorithme après",N, "itération est de", mean_accuracy_overfit, "\n")
```
La valeur d'accuracy calculée sur le jeu de données d'apprentissage est très bonne (99%) ce qui prouve que le modèle est bien entrainé.
Les résulats sont très bons, on obtient une accuracy de 96% sur le jeu de données test, cette valeur est suffisament proche de celle calculée sur le jeu de données d'apprentissage pour conclure qu'il n'y a pas de surapprentissage. Les classes minoritaires sont enfin correctement prédites.

```{r}
plot(rf$err.rate[, 1], type = "l", xlab = "nombre d'arbres", ylab = "erreur OOB")
```

On confirme le choix de ntree=20 qui minimise l'erreur de prédiction. 

### Conclusion
Pour conclure sur cet algorithme des Random Forest, on peut dire que celui-ci est très performant. Le temps de simulation est plus long mais on obtient une accuracy de 96% ce qui est très satisfaisant. De plus, avec la totalité des données, l'algorithme est plus précis et distingue même des classes minoritaires qui ressemblent à des classes majoritaires. 
Les résultats de la validation croisée montrent qu'il n'y a pas de surapprentissage. 

## Algorithmes model-driven

La méthode model-driven étudiée est le modèle multinomial. 

### Formalisme du modèle multinomial:

$$ Cover \: type \: \sim \: Elevation \: + \: Aspect \: + \: Slope \: + \: ... \: + \: Soil \: type \: 40 \: + \: Interactions$$

$Y = Cover \: type$, la variable réponse\
$Xs$ les variables prédictives\
Le but de cette méthode est de minimiser l'erreur en choisissant des coefficients apropriés.

### Analyse des interactions entre les variables X

Pour estimer quelles intéractions sont à inclure dans le modèle, on peut représenter les variables quantitatives X en fonction des variables X qualitatives

```{r}
# variables X qualitatives
temp_forest <- Xforest
temp_forest[,11:54][temp_forest[,11:54] == 0] <- NA

temp_forest <- pivot_longer(temp_forest,cols = seq(15,54),names_to = "Soil_type",values_to = "pres_abs", values_drop_na = TRUE)
temp_forest <- pivot_longer(temp_forest,cols = seq(11,14),names_to = "Wilderness_area",values_to = "pres_ab", values_drop_na = TRUE)
temp_forest <- temp_forest[,-c(12,14)]

# Interactions possibles entre les variables qualitatives et quantitatives

for (i in 1:10) {
  par(mfrow=c(1,2))
  boxplot(temp_forest[,i]~temp_forest$Wilderness_area, varwidth = TRUE, ylab = colnames(temp_forest)[i], xlab = "Wilderness area", main = "", xaxt = "n")
 boxplot(temp_forest[,i]~temp_forest$Soil_type, varwidth = TRUE, ylab = colnames(temp_forest)[i], xlab = "Soil type ", main = "", xaxt = "n")
}

rm(temp_forest)


```


Interactions potentielles :\
- Horizontal distance to fire, Wilderness area\
- Horizontal distance to Roadway, Wilderness area\
- Slope, Wilderness area\
- Slope, soil type\
- Elevation, Wilderness area

### Analyse des corrélations entre les X quantitatifs

Les modèles linéaire généralisés sont sensibles aux variables explicatives trop correlées entre elles, il faut donc vérifier les corrélations.

```{r}
M<-cor(Xforest[,1:10])
corrplot.mixed(M,upper="square",lower.col="black", tl.col="black",cl.cex = 0.7,tl.cex = 0.6,number.cex =0.7)
Xforest_red <- Xforest[,-9]
```

L'ombre à 3h est corrélée avec un coefficient r=-0.78 avec l'ombre à 9h. Si on prend un seuil à 0.7 on retire l'ombre à 3h.

### Centrer et réduire les variables quantitatives

```{r}
Xforest_red_cr <- Xforest_red
Xforest_red_cr[1:9] <- apply(Xforest_red_cr[1:9],MARGIN = 2,function(X) scale(X,center = TRUE, scale = TRUE))
```

### Modèle

-   Le modèle linéaire ne peut pas tourner sur 387341 données, donc on fait plusieurs fois tourner sur environ 4000 données. Dans l'idéal on le fait tourner environ 85 fois pour que le total des données d'entraînement représente environ 2/3 des données. Puis on devrait tester les predictions moyennes sur les 85 sous modèles sur le tier restant des données. Dans la pratique, on ne le fait tourner que dix fois car la puissance de nos ordinateurs est trop faible. \

-   Le plan n'est pas orthogonal, il est alors difficile d'utiliser d'interactions. On en utilise mais on fait attention aux coefficients prédits pour les interactions (Beta) qui peuvent être moins fiables surtout sur les modalités rares.\

-   On sélectionne les variables en ajouttant une pénalité lasso au modèle.

-   Pour tester la qualité du modèle on s'intéresse à l'accuracy moyenne, soit le nombre moyen de cover types correctement prédits sur l'ensemble des modèles simulés divisé par le nombre total de cover type testé.

### Ecriture du modèle


```{r fullmodel,include=TRUE}


rep <- 10 #normalement sur 550 essais mais tournerait trop longtemps
L_coeffs <- list()
prediction <- matrix(NA,nrow = 2066,ncol = rep*2)


T1<-Sys.time()
for (b in 1:rep) { 
  # subset de données 
  set.seed(b)
  select_indiv <- part_data(forest,freq_cover)
  Ytrain <- Cov_type[select_indiv[[1]]] 
  Ytest <- Cov_type[select_indiv[[2]]] 
  Xtrain <- Xforest_red_cr[select_indiv[[1]],] 
  Xtest <- Xforest_red_cr[select_indiv[[2]],]
  
  # Introduction des interactions dans le modèle
  f <- as.formula(Ytrain ~ .+Wilderness_Area_1:Elevation+
                    Wilderness_Area_2:Elevation+
                    Wilderness_Area_3:Elevation+
                    Wilderness_Area_4:Elevation+
                    Wilderness_Area_1:Slope+
                    Wilderness_Area_2:Slope+
                    Wilderness_Area_3:Slope+
                    Wilderness_Area_4:Slope+
                    Wilderness_Area_1:Horizontal_Distance_To_Fire_Points+
                    Wilderness_Area_2:Horizontal_Distance_To_Fire_Points+
                    Wilderness_Area_3:Horizontal_Distance_To_Fire_Points+
                    Wilderness_Area_4:Horizontal_Distance_To_Fire_Points+
                    Wilderness_Area_1:Horizontal_Distance_To_Roadways+
                    Wilderness_Area_2:Horizontal_Distance_To_Roadways+
                    Wilderness_Area_3:Horizontal_Distance_To_Roadways+
                    Wilderness_Area_4:Horizontal_Distance_To_Roadways,
                  cbind(Xtrain,Ytrain))
  
  f2 <- as.formula(Ytest ~ .+Wilderness_Area_1:Elevation+
                    Wilderness_Area_2:Elevation+
                    Wilderness_Area_3:Elevation+
                    Wilderness_Area_4:Elevation+
                    Wilderness_Area_1:Slope+
                    Wilderness_Area_2:Slope+
                    Wilderness_Area_3:Slope+
                    Wilderness_Area_4:Slope+
                    Wilderness_Area_1:Horizontal_Distance_To_Fire_Points+
                    Wilderness_Area_2:Horizontal_Distance_To_Fire_Points+
                    Wilderness_Area_3:Horizontal_Distance_To_Fire_Points+
                    Wilderness_Area_4:Horizontal_Distance_To_Fire_Points+
                    Wilderness_Area_1:Horizontal_Distance_To_Roadways+
                    Wilderness_Area_2:Horizontal_Distance_To_Roadways+
                    Wilderness_Area_3:Horizontal_Distance_To_Roadways+
                    Wilderness_Area_4:Horizontal_Distance_To_Roadways,
                  cbind(Xtest,Ytest))
  
  Xtrain_inter <- model.matrix(f, cbind(Xtrain,Ytrain))[, -1]
  Xtest_inter <- model.matrix(f2, cbind(Xtest,Ytest))[, -1]
  # formulation du modèle avec une pénalité lasso
  
  #Xtrain <- matrix(as.numeric(as.matrix(Xtrain)),nrow = nrow(Xtrain)) #pour un modèle sans intéractions
  mod <- cv.glmnet(Xtrain_inter,Ytrain, alpha=1,family = "multinomial")
  L_coeffs[[b]] <- coef(mod, s = "lambda.min") #on choisit la pénalité lasso optimale pour le modèle basé sur 
  
  # prédictions du modèle
  prediction[,b] <- predict(mod, Xtest_inter,type = "class",s = "lambda.min")
  prediction[,(rep*2-b+1)] <- Ytest



}

T2<-Sys.time()

(Tdiff= difftime(T2, T1))


```

On utilise une fonction qui optimize automatiquement la valeur du tuning parameter pour la pénalité lasso.

Plusieurs difficultés ont été rencontrées lors de cette étape: \ 
1- Comment choisir la pénalité lasso: test avec l'AIC, le BIC, otimisation par R. \ 
2- Combien d'intéractions intégrer dans le modèle, plus il y avait d'intéractions, plus le temps de calcul augmentait. Nous avons sélectionnées quelques interactions en guise d'exemple. \ 
3- Comment interpréter les interactions sachant que le plan n'est pas orthogonnal \ 
4- Commment faire l'échantillonnage: devons nous échantillonner le même nombre d'individus dans chaque cover type ou devons nous respecter les fréquences du jeu de donnée total? Après avoir testé les deux méthodes nous avons conservé la seconde car l'accuracy étaient bien meilleurs.


```{r}

df_mod <- matrix(c(rep(seq(1,7),7),
                   rep(1,7),
                   rep(2,7),
                   rep(3,7),
                   rep(4,7),
                   rep(5,7),
                   rep(6,7),
                   rep(7,7),rep(0,49)),byrow = FALSE,nrow = 49,ncol = 3)
df_mod <- as.data.frame(df_mod)
colnames(df_mod) <- c("Observed","Predicted","Freq")

confusion_mod <- NA
accuracy_mod <- c()

for (i in 1:rep) {
  mean_accuracy <- 0
  
  confusion_mod = as.data.frame((conf_mat = table(prediction[,((rep*2)-i+1)],prediction[,i], dnn=list("Observed","Predicted"))))
  confusion_mod$Observed <- as.numeric(confusion_mod$Observed)
  confusion_mod$Predicted <- as.numeric(confusion_mod$Predicted)
  for (j in 1:max(confusion_mod$Observed)) {
    for (k in 1:max(confusion_mod$Predicted)) {
      df_mod[df_mod$Observed == j & df_mod$Predicted == k,3] <- df_mod[df_mod$Observed == j & df_mod$Predicted == k,3]+ confusion_mod[confusion_mod$Observed == j & confusion_mod$Predicted == k,3]
      df_mod <- as.data.frame(df_mod)
    }
    #calcul de l'accuracy
    mean_accuracy <- mean_accuracy + confusion_mod[confusion_mod$Observed==j & confusion_mod$Predicted==j,3]
    }
    accuracy_mod<-c(accuracy_mod,mean_accuracy/2066)
    
  
}
df_mod_pourc<-df_mod
df_mod_pourc$Freq <-df_mod_pourc$Freq/rep
  
ggplot(df_mod_pourc, aes(x = Observed, y = Predicted, fill = Freq))+
  geom_tile()+
  geom_text(aes(label = round(Freq, 1))) +
  scale_fill_gradient(low="#FFFF88",high="red")+
  coord_fixed()+
  theme_minimal()+ggtitle("Matrice de confusion moyenne modèle multinomiale")+
  theme(axis.text = element_text(size = 12))+
  theme(axis.title = element_text(size = 14))+
  theme(plot.title = element_text(hjust = 0.5))




print(paste("Sur dix répétitions, l'accuracy est de :", mean(accuracy_mod)," et d'écart type, ", sd(accuracy_mod)))



```

Il semble y avoir beaucoup de confusion entre les classes 1,2 et 7. La classe 3 est plutôt bien prédite. Les classes les plus rares sont mal prédites: 4 est assignée à de nombreuses autres classes, 5 est assignée à la classe 2 et 6 aux classes 2 et 3. 

Lorsque l'échantillonnage imposait un nombre d'individus identique pour chaque type de couverture, on observait une sur-prédiction des classes rares.

On regarde les variables qui ressortent moins de 80% des fois

```{r}
occurence_var <- matrix(0,nrow = 70,ncol=7) # contient en ligne la somme des coefficients Beta non nuls au cours des répétitions du modele pour chaque classe en colonne

for (i in 1:rep) {
  for (j in 1:7) {
    occurence_var[(which(L_coeffs[[i]][[j]] != 0)),j] <- (occurence_var[(which(L_coeffs[[i]][[j]] != 0)),j]+ 1)
  }
  
}
#quelles variables ne sont pas présentes plus de 80% des fois ? 
occurence_var_pourcent <- apply(occurence_var, c(1,2), function(X) X*100/rep)
occurence_inf80 <- list()
for (i in 1:7) {
  occurence_inf80[[i]] <- c("intercept",row.names(L_coeffs[[1]]$`1`))[which(occurence_var_pourcent[,i]<80)]
}


```


![](\selectvar.png)

Représentées en bleu, les variables qui entrent en compte dans la prédiction des types de couvertures (de 1 à 7) au  moins 80% des fois, en rouge les autres. La colonne de droite correspond aux variables utilisées dans l'algorithme de random forest, en rouge celle dont la contribution est inférieure à 100 et en bleu celle dont elle est supérieure à 100. Les variables en plus claires représentent les variables propres à la régression linéaire soit: les interactions et l'intercept. \ 
  
  

Pour le modèle linéaire, les variables semblent quasiment toutes jouer un rôle dans la prédiction d'au moins une des classes, excepté les variables de type de sol 26 et 28, et l'interaction entre Horizontal distance to roadway et wilderness area?. Sans oublier la variable ombre à 3 heures retirée précedemment.  
Trois variables ont de l'importance pour au moins 5 des type de couvert: horizontal distance to roadway, ainse que les intéractions elevation:wilderness area 2 et horizontal distance to roadway: wilderness area 4. \ 

On ne remarque pas de motif particulier dans les variables importantes et pas de similitude avec les variables sélectionnées par le random forest.



### Attention à l'interprétation des coefficients d'intéractions

Pour les deux intéractions importantes, on regarde si les coefficients varient selon les simulations. On fait attention à cela notamment parce que le plan n'est pas orthognoal

```{r}
el_wa <- matrix(NA,nrow = rep,ncol = 7)
hdt_wa <- matrix(NA,nrow = rep,ncol = 7)
for (i in 1:10) {
  for (j in 1:7) {
    el_wa[i,j] <- L_coeffs[[i]][[j]][56] #coefficient de l'interaction elevation, wilderness area 2
    hdt_wa[i,j] <- L_coeffs[[i]][[j]][70] #coefficient de l'interaction horizontal distance to roadway, wilderness area 4
  }
  
}

interactions <- cbind(apply(el_wa,MARGIN = 2, mean),
      apply(el_wa,MARGIN = 2, sd),
      apply(hdt_wa,MARGIN = 2, mean),
      apply(hdt_wa,MARGIN = 2, sd))

colnames(interactions)<- c("moyenne Elevation:W area 2","ecart type Elevation:W area","moyenne Distance to roadway:W area 4","ecart type Distance to roadway:W area 4")

row.names(interactions)<- c("Cover type 1","Cover type 2","Cover type 3","Cover type 4","Cover type 5","Cover type 6","Cover type 7")

print(interactions)

```

Les écarts types des coefficients entre chaque simulations sont très grands, il devent donc difficile d'interpréter ces coefficients. 

# Comparaison des méthodes et conclusion


| Méthode       | Accurracy   | Remarques                                  |
|---------------|-------------|--------------------------------------------|
| K-means       | 0.55        | Toutes les classes ne sont pas prédites... |
| K-NN          | 0.37 à 0.57 | Résulats moyens                            |
| Multinomial model | 0.72    |                                            |
| Decision tree | 0.72 à 0.87 |                                            |
| Random forest | 0.78 à 0.98 |                                            |


Après avoir testé différentes approches dans le but de maximiser l'accuracy des prédictions des Cover_Type, on constate que la précision va de 55% à 96%. Lorsque l'on travaille avec le sous jeu de données, on constate que les méthodes arbre de décision, random et regression multinomiale donnent des résultats relativement proche, autour de 75% d'accuracy. Seule la méthode des KNN est moins efficace.La réduction de la dimension suite à la PCA a probablement engendré une perte d'information et donc une diminution de la précision de l'algorithme. Pour les data driven méthodes, les arbres de décisions et les Random forest, c'est le fait de travailler avec un nombre conséquent de données qui permet d'améliorer l'accuracy et ainsi d'obtenir 96% de bonnes prédiction pour les Random Forest. A cause d'un temps de simulation trop élevé, nous avons pas pu simuler la regression multinomiale avec toutes les données, cela équivaut à simuler 83 fois le modèle présenté. Paralléliser et ainsi diminuer le temps de simulation aurait pu permettre de comparer ce résutlat à celui des Random Forest et ainsi de savoir lequels des deux algorithmes est le plus efficace en terme de prédiction.  
  Dans la continuité de la regression multinomiale, nous aurions pu réaliser un réseau de neurones. Cependant, l'article associé à ce jeu de données a utilisé cette approche et a obtenu une accuracy moyenne de 71%. Les auteurs se disent deçus de ce résultat, et pense que cela provient de la diversité des variables (qualitative et quantitative), du déséquilibre de fréquence des Cover_Type et de leur difficulté à exploiter tout le jeu de données au vu des temps de simulations.
  
  Ainsi, malgré le fait que le jeu de données soit conséquent et nous ai posé des problèmes pour l'exploration de différents scénarios, la quantité de données est finalement le point fort qui permet d'obtenir cette accuracy de 96% avec les Random Forest. Ce jeu de données qui a la particularité d'être conséquent, rend donc l'algorithme des Random Forest, une data driven method, le plus performant tout en offrant un temps de simulation raisonnable. 
  Si le jeu de données avait été moins conséquent, une approche data driven n'aurai peut etre pas été la plus performante. Une approche model driven aurait peut être été plus efficace, notamment pour prédire les classes minoritaires. 


